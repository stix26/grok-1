name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

env:
  PYTHON_VERSION: '3.11'
  CUDA_VERSION: '12.1'

jobs:
  # Code Quality and Linting
  lint:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install ruff black isort mypy bandit safety
    
    - name: Run Ruff linter
      run: |
        ruff check .
        ruff format --check .
    
    - name: Run Black formatter check
      run: black --check --diff .
    
    - name: Run isort check
      run: isort --check-only --diff .
    
    - name: Run MyPy type checking
      run: mypy --ignore-missing-imports .
    
    - name: Run Bandit security linter
      run: bandit -r . -f json -o bandit-report.json || true
    
    - name: Run Safety check
      run: safety check --json --output safety-report.json || true

  # Unit Tests
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-mock pytest-asyncio
    
    - name: Run tests with pytest
      run: |
        pytest --cov=. --cov-report=xml --cov-report=html --cov-report=term-missing
      env:
        PYTHONPATH: .
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  # Security Scanning
  security:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'
    
    - name: Run Semgrep security scan
      uses: returntocorp/semgrep-action@v1
      with:
        config: >-
          p/security-audit
          p/secrets
          p/owasp-top-ten
        output-format: sarif
        output-file: semgrep-results.sarif
    
    - name: Upload Semgrep results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'semgrep-results.sarif'

  # Dependency Check
  dependency-check:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pip-audit
    
    - name: Run pip-audit
      run: pip-audit --format json --output pip-audit-report.json || true
    
    - name: Check for outdated dependencies
      run: |
        pip list --outdated --format=freeze > outdated-deps.txt
        if [ -s outdated-deps.txt ]; then
          echo "Found outdated dependencies:"
          cat outdated-deps.txt
          exit 1
        fi

  # Build and Test with Docker
  docker:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Create Dockerfile
      run: |
        cat > Dockerfile << 'EOF'
        FROM python:3.11-slim
        
        WORKDIR /app
        
        # Install system dependencies
        RUN apt-get update && apt-get install -y \
            build-essential \
            && rm -rf /var/lib/apt/lists/*
        
        # Copy requirements and install Python dependencies
        COPY requirements.txt .
        RUN pip install --no-cache-dir -r requirements.txt
        
        # Copy application code
        COPY . .
        
        # Create non-root user
        RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
        USER appuser
        
        # Health check
        HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
          CMD python -c "import sys; sys.exit(0)" || exit 1
        
        CMD ["python", "run.py"]
        EOF
    
    - name: Build Docker image
      run: docker build -t grok-1:test .
    
    - name: Run Docker container test
      run: |
        docker run --rm -d --name grok-1-test grok-1:test
        sleep 10
        docker ps | grep grok-1-test
        docker stop grok-1-test
    
    - name: Run Docker security scan
      run: |
        docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
          aquasec/trivy image grok-1:test --format json --output docker-security-scan.json || true

  # Performance and Load Testing
  performance:
    runs-on: ubuntu-latest
    needs: [test]
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install locust memory-profiler psutil
    
    - name: Create performance test script
      run: |
        cat > performance_test.py << 'EOF'
        import time
        import psutil
        import os
        
        def test_memory_usage():
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            print(f"Memory usage: {memory_info.rss / 1024 / 1024:.2f} MB")
            return memory_info.rss
        
        def test_import_performance():
            start_time = time.time()
            try:
                from model import LanguageModelConfig, TransformerConfig
                from runners import InferenceRunner, ModelRunner
                import_time = time.time() - start_time
                print(f"Import time: {import_time:.4f} seconds")
                return import_time
            except Exception as e:
                print(f"Import failed: {e}")
                return None
        
        if __name__ == "__main__":
            print("Running performance tests...")
            test_import_performance()
            test_memory_usage()
        EOF
    
    - name: Run performance tests
      run: python performance_test.py

  # Documentation Check
  documentation:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install documentation tools
      run: |
        python -m pip install --upgrade pip
        pip install pydocstyle doc8 sphinx sphinx-rtd-theme
    
    - name: Check docstring style
      run: pydocstyle . || true
    
    - name: Check README formatting
      run: doc8 README.md || true
    
    - name: Validate markdown files
      run: |
        pip install markdown-link-check
        find . -name "*.md" -exec markdown-link-check {} \; || true

  # Integration Tests
  integration:
    runs-on: ubuntu-latest
    needs: [test]
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Create integration test
      run: |
        cat > test_integration.py << 'EOF'
        import unittest
        import tempfile
        import os
        
        class TestIntegration(unittest.TestCase):
            def test_model_import(self):
                """Test that all model components can be imported"""
                try:
                    from model import LanguageModelConfig, TransformerConfig
                    from runners import InferenceRunner, ModelRunner
                    self.assertTrue(True)
                except ImportError as e:
                    self.fail(f"Failed to import model components: {e}")
            
            def test_config_creation(self):
                """Test that model config can be created"""
                try:
                    from model import LanguageModelConfig, TransformerConfig
                    
                    config = LanguageModelConfig(
                        vocab_size=128 * 1024,
                        pad_token=0,
                        eos_token=2,
                        sequence_len=8192,
                        embedding_init_scale=1.0,
                        output_multiplier_scale=0.5773502691896257,
                        embedding_multiplier_scale=78.38367176906169,
                        model=TransformerConfig(
                            emb_size=48 * 128,
                            widening_factor=8,
                            key_size=128,
                            num_q_heads=48,
                            num_kv_heads=8,
                            num_layers=64,
                            attn_output_multiplier=0.08838834764831845,
                            shard_activations=True,
                            num_experts=8,
                            num_selected_experts=2,
                            data_axis="data",
                            model_axis="model",
                        ),
                    )
                    self.assertIsNotNone(config)
                except Exception as e:
                    self.fail(f"Failed to create model config: {e}")
            
            def test_file_structure(self):
                """Test that required files exist"""
                required_files = [
                    'model.py',
                    'runners.py',
                    'run.py',
                    'requirements.txt',
                    'README.md',
                    'LICENSE.txt'
                ]
                
                for file in required_files:
                    self.assertTrue(os.path.exists(file), f"Required file {file} not found")
        
        if __name__ == '__main__':
            unittest.main()
        EOF
    
    - name: Run integration tests
      run: python -m pytest test_integration.py -v

  # Final Summary
  summary:
    runs-on: ubuntu-latest
    needs: [lint, test, security, dependency-check, docker, performance, documentation, integration]
    if: always()
    steps:
    - name: CI Summary
      run: |
        echo "## CI Pipeline Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Completed Jobs:" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Linting and Code Quality" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Unit Tests" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Security Scanning" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Dependency Check" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Docker Build and Test" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Performance Testing" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Documentation Check" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Integration Tests" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Next Steps:" >> $GITHUB_STEP_SUMMARY
        echo "1. Review any security findings" >> $GITHUB_STEP_SUMMARY
        echo "2. Address any linting issues" >> $GITHUB_STEP_SUMMARY
        echo "3. Update dependencies if needed" >> $GITHUB_STEP_SUMMARY
        echo "4. Consider adding more comprehensive tests" >> $GITHUB_STEP_SUMMARY 